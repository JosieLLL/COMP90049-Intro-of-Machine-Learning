{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP90049 Introduction to Machine Learning, 2020 Semester 2\n",
    "\n",
    "## Week 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, we will be using scikit-learn to classify some data, and to evaluate some classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.\n",
    "Please load Car Evaluation dataset from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data).\n",
    "\n",
    "The common terminology in scikit-learn is that the array defining the attribute values is called X and the array defining the gold–standard (“ground truth”) labels is called y ; create these variables for the car data.\n",
    "\n",
    "- **(a)** Load the data into a suitable format for scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "with open('car.data', mode='r') as fin:\n",
    "    for line in fin:\n",
    "        atts = line.strip().split(\",\")\n",
    "        X.append(atts[:-1]) #all atts, excluding the class\n",
    "        y.append(atts[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** How many instances are there in this collection? How many attributes, and of what type(s)? What is the class we’re trying to predict, and how many values does it take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1728 instances\n",
      "There are 6 attributes, for example: ['vhigh', 'vhigh', '2', '2', 'small', 'low']\n",
      "There are 4 class labels: {'vgood', 'acc', 'good', 'unacc'}\n",
      "Label frequencies: [('unacc', 1210), ('acc', 384), ('good', 69), ('vgood', 65)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print('There are', len(X), 'instances')\n",
    "print('There are', len(X[0]), \"attributes, for example:\", X[0])\n",
    "print('There are', len(set(y)), \"class labels:\", set(y))   \n",
    "#use Counter to count the number of labels\n",
    "label_counter = Counter(y)\n",
    "print(\"Label frequencies: %s\" %str(label_counter.most_common()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Unfortunately, scikit-learn isn’t set up to deal with our attributes in this format.\n",
    "\n",
    "- **(a)** Write some functions that transform our **categorical** attributes into **numerical** attributes, by (perhaps arbitrarily) assigning each categorical value to an integer, for example:\n",
    "\n",
    "```python\n",
    "def convert_class(raw):\n",
    "    if raw==\"unacc\": return 0\n",
    "    elif raw==\"acc\": return 1\n",
    "    elif raw==\"good\": return 2\n",
    "    elif raw==\"vgood\": return 3\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature 1: {'high', 'vhigh', 'med', 'low'}\n",
      "feature 2: {'high', 'vhigh', 'med', 'low'}\n",
      "feature 3: {'5more', '2', '3', '4'}\n",
      "feature 4: {'more', '2', '4'}\n",
      "feature 5: {'big', 'small', 'med'}\n",
      "feature 6: {'high', 'med', 'low'}\n"
     ]
    }
   ],
   "source": [
    "# We could check this from the \"car.names\" file linked above\n",
    "# Here's one (somewhat inefficient) way of reading this from the data itself\n",
    "feature_1_values = set([X[i][0] for i in range(len(X))])\n",
    "feature_2_values = set([X[i][1] for i in range(len(X))])\n",
    "feature_3_values = set([X[i][2] for i in range(len(X))])\n",
    "feature_4_values = set([X[i][3] for i in range(len(X))])\n",
    "feature_5_values = set([X[i][4] for i in range(len(X))])\n",
    "feature_6_values = set([X[i][5] for i in range(len(X))])\n",
    "print(\"feature 1: %s\" %str(feature_1_values))\n",
    "print(\"feature 2: %s\" %str(feature_2_values))\n",
    "print(\"feature 3: %s\" %str(feature_3_values))\n",
    "print(\"feature 4: %s\" %str(feature_4_values))\n",
    "print(\"feature 5: %s\" %str(feature_5_values))\n",
    "print(\"feature 6: %s\" %str(feature_6_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1728, 6), y shape: (1728,)\n",
      "[[3 3 0 0 0 0]\n",
      " [3 3 0 0 0 1]\n",
      " [3 3 0 0 0 2]\n",
      " ...\n",
      " [0 0 3 2 2 0]\n",
      " [0 0 3 2 2 1]\n",
      " [0 0 3 2 2 2]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_feature_1and2and6(raw):\n",
    "    if raw == \"low\": return 0\n",
    "    elif raw == \"med\": return 1\n",
    "    elif raw == \"high\": return 2\n",
    "    elif raw == \"vhigh\": return 3\n",
    "    # In general, we might want to catch unexpected values, too\n",
    "def convert_feature_3(raw):\n",
    "    if raw == \"2\": return 0\n",
    "    elif raw == \"3\": return 1\n",
    "    elif raw == \"4\": return 2\n",
    "    elif raw == \"5more\": return 3\n",
    "def convert_feature_4(raw):\n",
    "    if raw == \"2\": return 0\n",
    "    elif raw == \"4\": return 1\n",
    "    elif raw == \"more\": return 2\n",
    "def convert_feature_5(raw):\n",
    "    if raw == \"small\": return 0\n",
    "    elif raw == \"med\": return 1\n",
    "    elif raw == \"big\": return 2\n",
    "def convert_class(raw):\n",
    "    if raw == \"unacc\": return 0\n",
    "    elif raw == \"acc\": return 1\n",
    "    elif raw == \"good\": return 2\n",
    "    elif raw == \"vgood\": return 3\n",
    "\n",
    "X_ordinal = []\n",
    "for x in X:\n",
    "    f1, f2, f3, f4, f5, f6 = x\n",
    "    f1 = convert_feature_1and2and6(f1)\n",
    "    f2 = convert_feature_1and2and6(f2)\n",
    "    f3 = convert_feature_3(f3)\n",
    "    f4 = convert_feature_4(f4)\n",
    "    f5 = convert_feature_5(f5)\n",
    "    f6 = convert_feature_1and2and6(f6)\n",
    "    x = [f1, f2, f3, f4, f5, f6]\n",
    "    X_ordinal.append(x)\n",
    "    \n",
    "#convert to int array to make sure everything is converted.\n",
    "X_ordinal = np.array(X_ordinal, dtype='int')\n",
    "\n",
    "#convert ys\n",
    "y_numeric = []\n",
    "for this_y in y:\n",
    "    this_y = convert_class(this_y)\n",
    "    y_numeric.append(this_y)\n",
    "\n",
    "y_num = np.array(y_numeric, dtype='int')\n",
    "\n",
    "print('X shape: {}, y shape: {}'.format(X_ordinal.shape, y_num.shape))\n",
    "print(X_ordinal) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** Load the dataset again, this time as integers. Observe that we can actually build a model using this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_ordinal, y_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(c)** Split the data into training (80%) and test sets (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1157, 6) X_test: (571, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split # Newer versions\n",
    "#from sklearn.cross_validation import train_test_split # Older versions\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ordinal, y_num, test_size=0.33)\n",
    "print('X_train: {} X_test: {}'.format(X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.\n",
    "Read up on different implementations of the Naive Bayes classifier in `sklearn.naive_bayes`. Which one do you think is most suitable for the dataset we have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(a)** Implement the Bernoulli Naive Bayes classifier and inspect its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BNB score 0.763573 \n",
      "BNB score 0.814361 \n",
      "BNB score 0.779335 \n",
      "Avg BNB score: 0.7857559836544074\n"
     ]
    }
   ],
   "source": [
    "import sklearn.naive_bayes as nb\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bnb_accs = []\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "for i in range(3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_ordinal, y_num, test_size=0.33, random_state=i)\n",
    "    \n",
    "    bnb.fit(X_train, y_train)\n",
    "    acc = bnb.score(X_test, y_test)\n",
    "    print(\"BNB score %f \" %acc)\n",
    "    bnb_accs.append(acc)\n",
    "    \n",
    "print('Avg BNB score: {}'.format(np.mean(bnb_accs)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.\n",
    "\n",
    "Read up on the implelentation of the KNN classifier in `sklearn.neighbors.KNeighborsClassifier` and the implementation of distance functions in `sklearn.neighbors.DistanceMetric`. Implement the KNN classifier \n",
    "- with Manhattan distance \n",
    "- inverse distance weighting\n",
    "- K=5\n",
    "\n",
    "**(a)** Play with different values of K and weighting strategies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN score 0.833625 \n",
      "KNN score 0.821366 \n",
      "KNN score 0.823117 \n",
      "Avg KNN score: 0.8260361938120256\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='distance', metric='hamming')\n",
    "knn_accs = []\n",
    "\n",
    "for i in range(3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_ordinal, y_num, test_size=0.33, random_state=i)\n",
    "    \n",
    "    knn.fit(X_train, y_train)\n",
    "    acc = knn.score(X_test, y_test)\n",
    "    print(\"KNN score %f \" %acc)\n",
    "    knn_accs.append(acc)\n",
    "    \n",
    "print('Avg KNN score: {}'.format(np.mean(knn_accs)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.\n",
    "The transformation of the data in Q2 implicitly creates ordinal attributes. At first glance, such a strategy does seem reasonable in light of the given values (such as *small, med, big*).\n",
    "A different strategy would be to `binarise` the attributes: to replace a categorical attribute having `m` values with `m binary attributes`. One way of doing this in scikit-learn is using the **OneHotEncoder** :\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X)\n",
    "X_trans = ohe.transform(X).toarray()\n",
    "```\n",
    "\n",
    "Note that this transformation should be done before we split the data into training and test sets. (Why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(a)** Check the shape of `X_trans` — how many attributes do we have now? Does this correspond to your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1728, 21)\n",
      "X[0]: ['vhigh', 'vhigh', '2', '2', 'small', 'low']\n",
      "X_trans[0]: [0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X_ordinal)\n",
    "X_trans = ohe.transform(X_ordinal).toarray()\n",
    "\n",
    "print(X_trans.shape)\n",
    "print('X[0]:', X[0])\n",
    "print('X_trans[0]:', X_trans[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** Split the dataset comprised of `one–hot attributes` into **train** and **test** sets. Compare the accuracy of the Bernoulli Naive Bayes model and KNN using ordinal attributes with the same model using `one–hot attributes`: are you surprised? What can we infer?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BNB score 0.8371278458844134\tKNN score 0.882661996497373\n",
      "BNB score 0.8914185639229422\tKNN score 0.8739054290718039\n",
      "BNB score 0.8581436077057794\tKNN score 0.8931698774080561\n",
      "BNB score 0.8546409807355516\tKNN score 0.8669001751313485\n",
      "BNB score 0.8879159369527145\tKNN score 0.8756567425569177\n",
      "\n",
      "Avg BNB score: 0.8658493870402802 \t (0.020739574561243358)\n",
      "Avg KNN score: 0.8784588441330998 \t (0.00890246236577151)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "bnb_accs = []\n",
    "bnb = BernoulliNB()\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='distance', metric='hamming')\n",
    "knn_accs = []\n",
    "\n",
    "for i in range(5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_trans, y_num, test_size=0.33, random_state=i)\n",
    "   \n",
    "    bnb.fit(X_train, y_train)\n",
    "    knn.fit(X_train, y_train)\n",
    "    acc = bnb.score(X_test, y_test)\n",
    "    kacc = knn.score(X_test, y_test\n",
    "                    )\n",
    "    print(f\"BNB score {acc}\\tKNN score {kacc}\")\n",
    "    bnb_accs.append(acc)\n",
    "    knn_accs.append(kacc)\n",
    "    \n",
    "print(f'\\nAvg BNB score: {np.mean(bnb_accs)} \\t ({np.std(bnb_accs)})')\n",
    "print(f'Avg KNN score: {np.mean(knn_accs)} \\t ({np.std(knn_accs)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few observations.\n",
    "\n",
    "*Both classifiers see a slight performance boost, and now perform very similarl. The standard deviation of BNB results is much higher than the variance of KNN.*\n",
    "\n",
    "*The Bernoulli NB classifier improves in performance -- which is unsurprising as only after 1-hot encoding the data distribution matches the assumptions of the classifier (namely, that the values for each feature were generated by the Bernoully distribution)*\n",
    "\n",
    "*At this point, we can also observe that scikit-learn's Bernoulli NB is to do ... something ... with non-binary attributes. Most classifiers will return SOME result, for the data that is fed in. Is the Bernoulli classifier on the ordinal data representation a useful machine learning model? Is it a valid machine learning model? Which model can we trust more in deployment? The questions are vital and only possible to answer with a sound understanding of the underlying algorithms.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
